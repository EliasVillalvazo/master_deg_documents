{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Activation Functions","provenance":[],"authorship_tag":"ABX9TyPkxCzLiOuhpZTEj4Fz2LeT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# What are Activation Functions?\n","\n","They apply a **non-linear** transformation and decide whether a neuron should be activated or not.\n","\n","Without activation layers, then a neural network is just a stacked linear regression model that is not suited for complex tasks\n","\n","Applied after each layer\n","\n","## Examples\n","\n","##### Step Function\n","1 if X is > Y , where Y is a certain value\n","0 Otherwise\n","\n","Not used\n","\n","#### Sigmoid Function\n"," 1 / (1 + e ^ (-x))\n","\n"," Typically used in the last layer of a binary classification\n","\n"," #### TanH\n","Scaled sigmoid function that goes between -1 and 1\n","\n"," 2 / (1 + e ^ (-2x)) - 1\n","\n"," #### ReLU\n","\n"," max(0, x) - Outputs zero for negative values, otherwise the same value\n","\n"," If you don't know what to use, use ReLU for hidden layers\n","\n"," #### Leaky ReLU\n","\n"," x if x >= 0\n","\n"," a * x otherwise\n","\n"," Solves the vanishing gradient problem \n","\n","\n"," #### Softmax function\n","\n"," Good in last layer for multi-class classification problems"],"metadata":{"id":"2dE05yXJUWpn"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"C-3LNSVXUPur","executionInfo":{"status":"ok","timestamp":1644613772430,"user_tz":360,"elapsed":6428,"user":{"displayName":"Elías Villalvazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVQ4S0H8zVau32I05WYndw_jTrzmD-ABeX352Mlvw=s64","userId":"15189374646433169476"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","source":["class NeuralNet(nn.Module):\n","  def __init__(self, input_size, hidden_size):\n","    super().__init__()\n","    self.linear1 = nn.Linear(input_size, hidden_size)\n","    self.linear2 = nn.Linear(hidden_size, 1)\n","    #Alternative: Define here layers for relu and other activation functions\n","    # Example\n","    #self.relu = nn.ReLU()\n","    # self.softmax = nn.Softmax()\n","\n","  def forward(self, x):\n","    out = torch.relu(self.linear1(x))\n","    out = torch.sigmoid(self.linear2(x))\n","    return out"],"metadata":{"id":"YqRBjvNknnQ6","executionInfo":{"status":"ok","timestamp":1644614031560,"user_tz":360,"elapsed":2,"user":{"displayName":"Elías Villalvazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVQ4S0H8zVau32I05WYndw_jTrzmD-ABeX352Mlvw=s64","userId":"15189374646433169476"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Uun4I71LoWT2"},"execution_count":null,"outputs":[]}]}